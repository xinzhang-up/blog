# RAG优化

## 提升索引准确性

* 优化文本解析过程, 比如去掉页眉页脚
* 优化chunk切分模式
  * 利用领域知识, 针对特定领域的文档, 利用领域专有知识进行更精准的切分. 如在法律文档中识别段落编号, 条款作为切分依据
  * 基于固定大小切分
  * 上下文感知, 切分时考虑前后文关系, 避免信息断裂, 可以通过保持特定句对或短语相邻, 或使用更复杂的算法识别并保留语义完整性. 最简单的做法是切分时保留前一句和后一句话. 也可以使用自然语言处理技术识别语义单元, 如通过句子相似度计算, 主题模型如LDA或BERT嵌入聚类来切分文本, 确保每个chunk内部语义连贯, 减少跨chunk信息依赖, 也可以考虑使用更复杂的切分策略, 如围绕关键词切分或采用动态调整的切分策略等
* 句子滑动窗口检索, 通过设置window_size来调整提取句子的数量, 当用户问匹配到一个chunk语料块时, 通过窗函数提取目标语料块的上下文
* 自动合并检索, 将文档切块, 建成一棵语料块的树, 如果同一个父节点的多个叶子节点被选中, 则返回整个父节点对应的语料块, 从而确保与问题相关的语料信息被完整保存下来
* 选择更适合业务的Embedding模型
* 选择更适合业务的ReRank模型, 可以考虑在ModelScope社区搜索最近关注度高, 下载量大的模型
* Raptor用聚类为文档块建立索引, 采用无监督聚类来生成文档索引

## 让问题更好理解

* Enrich完善用户问题, 如果LLM判断问题不完整, 则通过多轮对话逐步确认用户需求
* Multi-Query多路召回, 首先一次性改写出多种用户问题, 让大模型根据用户问, 从多种不同角度去生成有一定提问角度或提问内容上存在差异的问题, 然后再把每个问题分别生成答案, 并总结出最终答案
* RAG-Fusion过滤融合, 多路召回获取到各种语料块后, 先进行一轮筛选去重, 然后按与原问题的相关性进行排序, 再将语料打包给到大模型来生成答案, 用人类的例子讲就是先从多角度理解用户问, 然后检索资料, 再把重复信息去掉, 再将资料排序
* Step Back问题摘要, 让大模型先对问题进行一轮抽象, 从大体上去把握用户问, 获得一层高级思考下的语料块
* Decomposition问题分解, 类似COT, 将用户问拆成一个一个小问题来理解, 再使用串行或并行获取答案
* HyDE假设答案, 让大模型先根据用户问生成一段假设答案, 然后用这段假设的答案作为新的问题去文档库里匹配新的文档块, 再进行总结, 生成最终答案, 适合零样本场景下的稠密检索, 不需要人工标注数据, 弥补原始信息不足的问题

## 改造检索渠道

* 主动搜索互联网

## 回答前反复思考

* 自我反思, self-reflection, 让应用问自己三个问题
  * 相关性: 我获取的这些材料和问题相关吗?
  * 无幻觉: 我的答案是不是按照材料写的来讲, 还是我自己编造的?
  * 已解答: 我的答案是不是解答了问题?

## 从多种数据源获取资料

* 数据库-NL2SQL
* 向量数据库
* 图数据库-NL2Cypher
